import asyncio
from datetime import datetime
import math
import random
from typing import List
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import schemas
from models import MachineLearningData

#### We define the representativeness of an object as: 
#### 1/(1 + average_distance_of_k_neighbors)


NUMBER_OF_MODELS: int = 4    # "L" from task description - Default number of parts into which we split 
                             # the whole group of training arrays; also the number of models
                             # from which the averaged prediction will be obtained.
                             # It is _not_ directly editable by the User

SIZE: int = 5  # Default size of one array of numbers that will become a single row 
          # of independent variables in the training data; not editable by the User

ARRAYS_AMOUNT: int = 40  # Total number of arrays of numbers (each of the size = SIZE) 
                     # on which the model will be initially trained

K_NEAREST_NEIGHBOURS: int = 3 # the number of the nearest neighbours of each analyzed object - 
                          # this number will influence the representativeness analysis



# Function generating a set of arrays of the chosen size 'size' with arrays_amount elements;
# It returns a list of arrays of random numbers to be used as independent variables in ML training
def generate_arrays(size: int, 
                          #number of arrays must be above a number that provides 
                          #enough examples to train the model but is not excessively high
                          arrays_amount: int):

    
    # Initialize an empty list to store the generated arrays
    arrays = []
    
    # Loop over the specified number of arrays to generate
    for i in range(arrays_amount):
        # Generate an array of size 'size' comprising of numbers from 0 to 100
        array = [random.uniform(0, 100) for _ in range(size)]
        
        # Add the generated array to the list of arrays
        arrays.append(array)
    
    return arrays


generated_arrays = generate_arrays(SIZE, ARRAYS_AMOUNT) # all arrays of independent variables are generated


# Function takes the training_arrays list of training arrays generated by generate_arrays,
# and splits it randomly into NUMBER_OF_MODELS, keeping the order of the numbers in individual lists
def split_data_for_models(training_arrays, PartsNumber: int): #maybe change PartsNumber to local variable?
    
    # Create and shuffle indices
    indices = list(range(len(training_arrays)))
    random.shuffle(indices)

    # Split data using shuffled indices
    split_data = [[] for _ in range(PartsNumber)]
    for i, idx in enumerate(indices):
        split_data[i % PartsNumber].append(training_arrays[idx])
  
    split_data = [training_arrays[i::PartsNumber] for i in range(PartsNumber)]
    
    return split_data

split_training_arrays = split_data_for_models(generated_arrays, NUMBER_OF_MODELS)

# Function calculating representativeness values for all the objects in a given list
# and filling a dict of the calculated repr. values for each object
def repr_calc_in_a_set(objects: list):
    #For each object, calculating distances from all other objects
    SublistSize = len(objects)
    ReprDict = []# dictionary of pairs object : representativeness, with the size of the sublist taken by the function
    for position in range(SublistSize):
        # calculating the euclidean distance - squared difference of every pair of numbers
        obj = objects[position]
        distances_sum = 0
        neighbours = [element for element in objects if element != objects[position]]
        neighbours_with_distances = []
        #Calculating the distances to all neighbours to find the K nearest neighbours
        for neighbour in neighbours:
            distance = math.sqrt(sum((a - b)**2 for a, b in zip(obj, neighbour)))
            # adding up all the distances from the current considered object ot the rest of the neighbors
            neighbours_with_distances.append([neighbour, distance])
            
            # print(f"distance = {distance}")  # printing for visual checking of example results
        
        nearest_neighbours = sorted(neighbours_with_distances, key=lambda x: x[1], reverse=True)[:K_NEAREST_NEIGHBOURS]
        # adding up all K nearest neighbours' distances for the object, taking the average and calculating representativeness based on it
        distances_sum = sum(obj_and_dist[1] for obj_and_dist in nearest_neighbours) 
        avg_distance = distances_sum / K_NEAREST_NEIGHBOURS
        representativeness = 1 / (1 + avg_distance)
        # appending the representativeness value to a list of objects mapped to their repr. values
        ReprDict.append([obj, representativeness]) #appending mutable elements in order to enable feature scaling later
    print(f"list of representativeness:")
    print(ReprDict)
    return(ReprDict)

model_number = 0

# Splits the list of pairs in a MyModel object into two arrays.
def split_data(data):
 
    numbers_list = [pair[0] for pair in data]
    targets_list = [pair[1] for pair in data]
    print(f"Independent variables sets: {numbers_list}")
    print(f"Dependent variable sets: {targets_list}")
    return numbers_list, targets_list

for training_array in split_training_arrays:
    object_to_representation = repr_calc_in_a_set(training_array)
    print(f"analyzed object No. {model_number}")
    model_number+=1
    split_data(object_to_representation)


### WORKING AS INTENDED - SYNCHRONOUS VERSION (CHANGE TO ASYNC)


#Function trains the ML model using Random Forest Regression
async def train_model(dataset: MachineLearningData):  #Uses the libraries: numpy, matplotlib, pandas
   
    #Importing the dataset
    # getting the array of all independent variables into the X variable
    X = dataset.iloc[:, 0].values ### ! CHECK THIS LINE 
    
    # getting the array of all dependent variables into the y variable
    y = dataset.iloc[:, 1].values ### ! CHECK THIS LINE 

    #Splitting the dataset into the Training set and Test set
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

    #Training the Random Forest Regression model on the whole dataset
    from sklearn.ensemble import RandomForestRegressor
    regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)
    regressor.fit(X_train, y_train)

    #Predicting the Test set results
    y_pred = regressor.predict(X_test)
    np.set_printoptions(precision=2)
    print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

    #Evaluating the Model Performance
    from sklearn.metrics import r2_score
    r2_score(y_test, y_pred)
    







def get_training_status():
  """
  Returns a dictionary of informations on the status of the ML process.

  Returned values:
    status: "Error", "model training still in progress", "model training completed"
    error_message: an optional message about the error if it occured
    start_time, end_time: time when the model training started and when it ended
  """

  status = "model training still in progress"
  error_message = None
  start_time = _start_time

  if _training_failed:
    status = "Error"
    error_message = _error_message

  if _training_completed:
    status = "model training completed"
    end_time = _end_time

  return {
    "status": status,
    "error_message": error_message,
    "start_time": start_time.isoformat(),
    "end_time": end_time.isoformat() if end_time else None,
  }

# Zmienne do Å›ledzenia statusu
_training_failed = False
_error_message = None
_start_time = None
_training_completed = False
_end_time = None

# Function that initiates the training of the model.
def start_training():
  # setting the global variables for status observation
  global _training_failed, _error_message, _start_time, _training_completed, _end_time

  _training_failed = False
  _error_message = None
  _start_time = datetime.now()
  _training_completed = False
  _end_time = None
# Function to be called when the model trianing ends - it updates the status variables
def end_training(success=True):
  """
  Argumenty:
    success: True, if the training finished successfully , otherwise False. ### ! prepare the 'success' variable: it should check 
                                                                            ### if the full set of dependent variables is delivered
                                                                            ### at the end of the model training.
  """

  global _training_failed, _error_message, _training_completed, _end_time

  _training_completed = True 
  _end_time = datetime.now()

  if not success:
    _training_failed = True
    _error_message = "An error occurred during the model training."

